{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import esm\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "from dictionary import AutoEncoder\n",
    "from config import my_config\n",
    "from dataset import ProteinDataset, collate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM-2 model\n",
    "# esm_model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "norm_b = esm_model.emb_layer_norm_after.bias.clone().detach().requires_grad_(False).to(my_config['device'])\n",
    "norm_w = esm_model.emb_layer_norm_after.weight.clone().detach().requires_grad_(False).to(my_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae(sae_model, chk):\n",
    "    chk_path = f'/share/vault/Users/ch3849/esm_sae/model/{sae_model}/checkpoints/step_{chk}.pt'\n",
    "    sae = AutoEncoder.from_pretrained(chk_path)\n",
    "    sae.eval()  # disables dropout for deterministic results\n",
    "    return sae.to(my_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/share/vault/Users/ch3849/esm_sae/sequence/eval_test_seq_max1022_addmask_perplexity.csv', nrows=100)\n",
    "# df = df[df['split'] == 'eval'].reset_index(drop=True)\n",
    "stage = 'representative'\n",
    "\n",
    "dataset = ProteinDataset(df=df, df_name_col=my_config[f'df_name_col_{stage}'], embed_logit_path=my_config[f'embed_logit_path_{stage}'], stage=stage)\n",
    "loader = DataLoader(dataset, collate_fn=collate_batch, batch_size=60, drop_last=False, num_workers=10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data in loader:\n",
    "    act = batch_data['repr'].to(my_config[\"device\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (18952,40960) (0,40960) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m l1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(f)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     f_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     ratio[sae_model]\u001b[38;5;241m.\u001b[39mappend((f_diff \u001b[38;5;241m/\u001b[39m l1[k:])\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (18952,40960) (0,40960) "
     ]
    }
   ],
   "source": [
    "ratio = {250416: [], 250417: []}\n",
    "for sae_model in [250416, 250417]:\n",
    "    sae = get_sae(sae_model, 80000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f = sae.encode(act).cpu().numpy()\n",
    "        \n",
    "    l1 = abs(f).sum(axis=1)\n",
    "    for k in range(1,11):\n",
    "        f_diff = abs(f[k:] - f[:-k]).sum(axis=1)\n",
    "        ratio[sae_model].append((f_diff / l1[k:]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ratio).to_csv('/share/vault/Users/ch3849/esm_sae/fig/sae_basic/SAE_feature_diff_ratio.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to shuffle, move non-zero values to the top, and sort\n",
    "def process_tensor(tensor):\n",
    "    # Shuffle the tensor\n",
    "    flattened = tensor.flatten()\n",
    "    shuffled_indices = torch.randperm(flattened.numel())\n",
    "    shuffled_flattened = flattened[shuffled_indices]\n",
    "    shuffled_tensor = shuffled_flattened.view(tensor.size())\n",
    "    \n",
    "    # Create tensors for storing non-sorted and sorted results\n",
    "    rows, cols = shuffled_tensor.size()\n",
    "    non_sorted_tensor = torch.zeros_like(shuffled_tensor)\n",
    "    sorted_tensor = torch.zeros_like(shuffled_tensor)\n",
    "    \n",
    "    for col in range(cols):\n",
    "        # Extract non-zero values from the shuffled tensor\n",
    "        nonzeros = shuffled_tensor[:, col][shuffled_tensor[:, col] != 0]\n",
    "        \n",
    "        # Non-sorted version: Place non-zero values at the top\n",
    "        non_sorted_tensor[:len(nonzeros), col] = nonzeros\n",
    "        \n",
    "        # Sorted version: Sort non-zero values and place at the top\n",
    "        sorted_tensor[:len(nonzeros), col] = torch.sort(nonzeros)[0]\n",
    "    \n",
    "    return shuffled_tensor, non_sorted_tensor, sorted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_loss = (torch.cat([f[1:-2].unsqueeze(0), f[2:-1].unsqueeze(0), f[3:].unsqueeze(0)], dim=0) - f[0:-3].unsqueeze(0)).norm(p=1, dim=-1)\n",
    "smooth_loss = (softmin(nearby_loss)*nearby_loss).sum(dim=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39.2049, device='cuda:1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        nearby_loss = (torch.cat([f[1:-2].unsqueeze(0), f[2:-1].unsqueeze(0), f[3:].unsqueeze(0)], dim=0) - f[0:-3].unsqueeze(0)).norm(p=1, dim=-1).mean(dim=-1)\n",
    "        smooth_loss = self.softmin(nearby_loss)@nearby_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_loss = torch.tensor([(f_splice - f[:-3]).norm(p=1, dim=-1).mean(dim=-1) for f_splice in [f[1:-2], f[2:-1], f[3:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmin = torch.nn.Softmin(dim=0)\n",
    "# smooth_loss = softmin(nearby_loss)@nearby_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.1605)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "smooth_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.1040, 45.6899, 47.5280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearby_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49.5101, 47.5359, 42.4042], device='cuda:1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cat([f[:-3].unsqueeze(0), f[1:-2].unsqueeze(0), f[2:-1].unsqueeze(0)], dim=0) - f[3:].unsqueeze(0)).norm(p=1, dim=-1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4438, 40960])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[3:-3].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.4375, device='cuda:1')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neaby_f = torch.cat([f[:-6].unsqueeze(0), f[1:-5].unsqueeze(0), f[2:-4].unsqueeze(0), f[4:-2].unsqueeze(0), f[5:-1].unsqueeze(0), f[6:].unsqueeze(0)], dim=0)\n",
    "nearby_loss = (neaby_f - f[3:-3].unsqueeze(0)).norm(p=1, dim=-1).mean(dim=-1)\n",
    "softmin = torch.nn.Softmin(dim=-1)\n",
    "softmin(nearby_loss)@nearby_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.601, 2.07, 1.056, 0.688, 1.597, 2.06, 0.764, 0.495, 1.623, 2.083, 0.936, 0.551, 1.606, 2.077, 0.807, 0.402, 1.61, 2.081, 0.96, 0.583, 1.641, 2.073, 1.02, 0.758, 1.638, 2.072, 1.085, 0.69, 1.6, 2.103, 0.916, 0.615, 1.627, 2.086, 0.993, 0.647, 1.594, 2.062, 0.884, 0.547, 1.584, 2.091, 1.137, 0.72, 1.539, 2.08, 0.973, 0.58, 1.569, 2.086, 1.098, 0.669, 1.579, 2.071, 1.125, 0.77, 1.622, 2.066, 0.879, 0.588, 1.595, 2.08, 0.867, 0.495, 1.617, 2.084, 1.026, 0.714, 1.601, 2.075, 0.775, 0.46, 1.58, 2.07, 0.932, 0.61, 1.613, 2.082, 0.728, 0.503, 1.603, 2.085, 1.056, 0.723, 1.608, 2.078, 0.891, 0.613, 1.594, 2.089, 1.072, 0.698, 1.641, 2.09, 0.851, 0.575, 1.557, 2.101, 0.895, 0.548, 1.603, 2.079, 0.983, 0.632, 1.597, 2.076, 0.932, 0.587, 1.589, 2.07, 0.966, 0.659, 1.57, 2.086, 0.871, 0.465, 1.627, 2.072, 1.155, 0.738, 1.6, 2.074, 0.746, 0.493, 1.613, 2.077, 0.896, 0.517, 1.595, 2.097, 0.939, 0.727, 1.616, 2.09, 0.931, 0.478, 1.609, 2.067, 0.938, 0.526, 1.588, 2.084, 1.124, 0.657, 1.637, 2.065, 0.913, 0.623, 1.584, 2.071, 0.945, 0.557, 1.62, 2.09, 0.887, 0.592, 1.649, 2.068, 0.941, 0.539, 1.579, 2.077, 0.893, 0.634, 1.539, 2.082, 1.052, 0.639, 1.6, 2.082, 1.183, 0.744, 1.622, 2.064, 0.866, 0.557, 1.599, 2.085, 0.968, 0.638, 1.613, 2.07, 0.91, 0.559, 1.614, 2.073, 0.9, 0.629, 1.62, 2.072, 0.837, 0.635, 1.616, 2.07, 0.738, 0.422, 1.641, 2.085, 0.649, 0.394, 1.585, 2.077, 1.202, 0.653, 1.6, 2.08, 0.834, 0.52, 1.547, 2.073, 1.074, 0.714, 1.595, 2.081, 0.997, 0.607, 1.579, 2.068, 0.853, 0.518, 1.584, 2.079, 0.719, 0.445, 1.606, 2.073, 0.973, 0.527, 1.607, 2.073, 0.924, 0.522, 1.604, 2.075, 0.9, 0.549, 1.574, 2.087, 0.853, 0.496, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m smooth_loss_2 \u001b[38;5;241m=\u001b[39m (f[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m f[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# L2, RMSE\u001b[39;00m\n\u001b[1;32m     13\u001b[0m smooth_loss_3 \u001b[38;5;241m=\u001b[39m (f[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m f[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# L1\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m shuffled_f, non_sorted_f, sorted_f \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m smooth_loss_shuffle \u001b[38;5;241m=\u001b[39m (shuffled_f[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m shuffled_f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m shuffled_f[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# L1\u001b[39;00m\n\u001b[1;32m     19\u001b[0m smooth_loss_nonsorted \u001b[38;5;241m=\u001b[39m (non_sorted_f[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m non_sorted_f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m non_sorted_f[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# L1\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mprocess_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_tensor\u001b[39m(tensor):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Shuffle the tensor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     flattened \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m----> 7\u001b[0m     shuffled_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     shuffled_flattened \u001b[38;5;241m=\u001b[39m flattened[shuffled_indices]\n\u001b[1;32m      9\u001b[0m     shuffled_tensor \u001b[38;5;241m=\u001b[39m shuffled_flattened\u001b[38;5;241m.\u001b[39mview(tensor\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smooth_loss_ratio = []\n",
    "smooth_loss_shuffle_ratio = []\n",
    "smooth_loss_nonsorted_ratio = []\n",
    "smooth_loss_sorted_ratio = []\n",
    "\n",
    "\n",
    "for batch_data in loader:\n",
    "    with torch.no_grad():\n",
    "        act = batch_data['repr']\n",
    "        act = (act.to(my_config[\"device\"]) - norm_b) / norm_w\n",
    "        f = sae.encode(act)\n",
    "        l1_loss = f.norm(p=1, dim=-1)\n",
    "\n",
    "        smooth_loss_1 = ((f[2:]/2 + f[:-2]/2 - f[1:-1])**2).sum(dim=-1) # MSE\n",
    "        smooth_loss_2 = (f[2:]/2 + f[:-2]/2 - f[1:-1]).norm(p=2, dim=-1) # L2, RMSE\n",
    "        smooth_loss_3 = (f[2:]/2 + f[:-2]/2 - f[1:-1]).norm(p=1, dim=-1) # L1\n",
    "\n",
    "        shuffled_f, non_sorted_f, sorted_f = process_tensor(f)\n",
    "\n",
    "\n",
    "        smooth_loss_shuffle = (shuffled_f[2:]/2 + shuffled_f[:-2]/2 - shuffled_f[1:-1]).norm(p=1, dim=-1) # L1\n",
    "        smooth_loss_nonsorted = (non_sorted_f[2:]/2 + non_sorted_f[:-2]/2 - non_sorted_f[1:-1]).norm(p=1, dim=-1) # L1\n",
    "        smooth_loss_sorted = (sorted_f[2:]/2 + sorted_f[:-2]/2 - sorted_f[1:-1]).norm(p=1, dim=-1) # L1\n",
    "\n",
    "        smooth_loss_ratio.append((smooth_loss_3 / l1_loss[1:-1]).mean().item())\n",
    "        smooth_loss_shuffle_ratio.append((smooth_loss_shuffle / l1_loss[1:-1]).mean().item())\n",
    "        smooth_loss_nonsorted_ratio.append((smooth_loss_nonsorted / l1_loss[1:-1]).mean().item())\n",
    "        smooth_loss_sorted_ratio.append((smooth_loss_sorted / l1_loss[1:-1]).mean().item())\n",
    "\n",
    "    for i in [smooth_loss_3, smooth_loss_shuffle, smooth_loss_nonsorted, smooth_loss_sorted]:\n",
    "        print(round((i / l1_loss[1:-1]).mean().item(),3), end=', ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
